{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting google-cloud-bigquery\n",
      "  Using cached google_cloud_bigquery-3.30.0-py2.py3-none-any.whl.metadata (7.9 kB)\n",
      "Collecting google-api-core<3.0.0dev,>=2.11.1 (from google-api-core[grpc]<3.0.0dev,>=2.11.1->google-cloud-bigquery)\n",
      "  Using cached google_api_core-2.24.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting google-auth<3.0.0dev,>=2.14.1 (from google-cloud-bigquery)\n",
      "  Using cached google_auth-2.38.0-py2.py3-none-any.whl.metadata (4.8 kB)\n",
      "Collecting google-cloud-core<3.0.0dev,>=2.4.1 (from google-cloud-bigquery)\n",
      "  Using cached google_cloud_core-2.4.3-py2.py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting google-resumable-media<3.0dev,>=2.0.0 (from google-cloud-bigquery)\n",
      "  Using cached google_resumable_media-2.7.2-py2.py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: packaging>=20.0.0 in .\\venv\\lib\\site-packages (from google-cloud-bigquery) (24.2)\n",
      "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.3 in .\\venv\\lib\\site-packages (from google-cloud-bigquery) (2.9.0.post0)\n",
      "Collecting requests<3.0.0dev,>=2.21.0 (from google-cloud-bigquery)\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting googleapis-common-protos<2.0.0,>=1.56.2 (from google-api-core<3.0.0dev,>=2.11.1->google-api-core[grpc]<3.0.0dev,>=2.11.1->google-cloud-bigquery)\n",
      "  Using cached googleapis_common_protos-1.69.2-py3-none-any.whl.metadata (9.3 kB)\n",
      "Collecting protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.19.5 (from google-api-core<3.0.0dev,>=2.11.1->google-api-core[grpc]<3.0.0dev,>=2.11.1->google-cloud-bigquery)\n",
      "  Using cached protobuf-6.30.1-cp310-abi3-win_amd64.whl.metadata (593 bytes)\n",
      "Collecting proto-plus<2.0.0,>=1.22.3 (from google-api-core<3.0.0dev,>=2.11.1->google-api-core[grpc]<3.0.0dev,>=2.11.1->google-cloud-bigquery)\n",
      "  Using cached proto_plus-1.26.1-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting grpcio<2.0dev,>=1.33.2 (from google-api-core[grpc]<3.0.0dev,>=2.11.1->google-cloud-bigquery)\n",
      "  Using cached grpcio-1.71.0-cp313-cp313-win_amd64.whl.metadata (4.0 kB)\n",
      "Collecting grpcio-status<2.0.dev0,>=1.33.2 (from google-api-core[grpc]<3.0.0dev,>=2.11.1->google-cloud-bigquery)\n",
      "  Using cached grpcio_status-1.71.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth<3.0.0dev,>=2.14.1->google-cloud-bigquery)\n",
      "  Using cached cachetools-5.5.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth<3.0.0dev,>=2.14.1->google-cloud-bigquery)\n",
      "  Using cached pyasn1_modules-0.4.1-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth<3.0.0dev,>=2.14.1->google-cloud-bigquery)\n",
      "  Using cached rsa-4.9-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting google-crc32c<2.0dev,>=1.0 (from google-resumable-media<3.0dev,>=2.0.0->google-cloud-bigquery)\n",
      "  Using cached google_crc32c-1.7.0-py3-none-any.whl\n",
      "Requirement already satisfied: six>=1.5 in .\\venv\\lib\\site-packages (from python-dateutil<3.0dev,>=2.7.3->google-cloud-bigquery) (1.17.0)\n",
      "Collecting charset-normalizer<4,>=2 (from requests<3.0.0dev,>=2.21.0->google-cloud-bigquery)\n",
      "  Using cached charset_normalizer-3.4.1-cp313-cp313-win_amd64.whl.metadata (36 kB)\n",
      "Collecting idna<4,>=2.5 (from requests<3.0.0dev,>=2.21.0->google-cloud-bigquery)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests<3.0.0dev,>=2.21.0->google-cloud-bigquery)\n",
      "  Using cached urllib3-2.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests<3.0.0dev,>=2.21.0->google-cloud-bigquery)\n",
      "  Using cached certifi-2025.1.31-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.19.5 (from google-api-core<3.0.0dev,>=2.11.1->google-api-core[grpc]<3.0.0dev,>=2.11.1->google-cloud-bigquery)\n",
      "  Using cached protobuf-5.29.3-cp310-abi3-win_amd64.whl.metadata (592 bytes)\n",
      "Collecting pyasn1<0.7.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth<3.0.0dev,>=2.14.1->google-cloud-bigquery)\n",
      "  Using cached pyasn1-0.6.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Using cached google_cloud_bigquery-3.30.0-py2.py3-none-any.whl (247 kB)\n",
      "Using cached google_api_core-2.24.2-py3-none-any.whl (160 kB)\n",
      "Using cached google_auth-2.38.0-py2.py3-none-any.whl (210 kB)\n",
      "Using cached google_cloud_core-2.4.3-py2.py3-none-any.whl (29 kB)\n",
      "Using cached google_resumable_media-2.7.2-py2.py3-none-any.whl (81 kB)\n",
      "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Using cached cachetools-5.5.2-py3-none-any.whl (10 kB)\n",
      "Using cached certifi-2025.1.31-py3-none-any.whl (166 kB)\n",
      "Using cached charset_normalizer-3.4.1-cp313-cp313-win_amd64.whl (102 kB)\n",
      "Using cached googleapis_common_protos-1.69.2-py3-none-any.whl (293 kB)\n",
      "Using cached grpcio-1.71.0-cp313-cp313-win_amd64.whl (4.3 MB)\n",
      "Using cached grpcio_status-1.71.0-py3-none-any.whl (14 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached proto_plus-1.26.1-py3-none-any.whl (50 kB)\n",
      "Using cached protobuf-5.29.3-cp310-abi3-win_amd64.whl (434 kB)\n",
      "Using cached pyasn1_modules-0.4.1-py3-none-any.whl (181 kB)\n",
      "Using cached rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Using cached urllib3-2.3.0-py3-none-any.whl (128 kB)\n",
      "Using cached pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
      "Installing collected packages: urllib3, pyasn1, protobuf, idna, grpcio, google-crc32c, charset-normalizer, certifi, cachetools, rsa, requests, pyasn1-modules, proto-plus, googleapis-common-protos, google-resumable-media, grpcio-status, google-auth, google-api-core, google-cloud-core, google-cloud-bigquery\n",
      "Successfully installed cachetools-5.5.2 certifi-2025.1.31 charset-normalizer-3.4.1 google-api-core-2.24.2 google-auth-2.38.0 google-cloud-bigquery-3.30.0 google-cloud-core-2.4.3 google-crc32c-1.7.0 google-resumable-media-2.7.2 googleapis-common-protos-1.69.2 grpcio-1.71.0 grpcio-status-1.71.0 idna-3.10 proto-plus-1.26.1 protobuf-5.29.3 pyasn1-0.6.1 pyasn1-modules-0.4.1 requests-2.32.3 rsa-4.9 urllib3-2.3.0\n"
     ]
    }
   ],
   "source": [
    "! pip install  google-cloud-bigquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting kagglehub\n",
      "  Downloading kagglehub-0.3.10-py3-none-any.whl.metadata (31 kB)\n",
      "Requirement already satisfied: packaging in .\\venv\\lib\\site-packages (from kagglehub) (24.2)\n",
      "Collecting pyyaml (from kagglehub)\n",
      "  Downloading PyYAML-6.0.2-cp313-cp313-win_amd64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: requests in .\\venv\\lib\\site-packages (from kagglehub) (2.32.3)\n",
      "Collecting tqdm (from kagglehub)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in .\\venv\\lib\\site-packages (from requests->kagglehub) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in .\\venv\\lib\\site-packages (from requests->kagglehub) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in .\\venv\\lib\\site-packages (from requests->kagglehub) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in .\\venv\\lib\\site-packages (from requests->kagglehub) (2025.1.31)\n",
      "Requirement already satisfied: colorama in .\\venv\\lib\\site-packages (from tqdm->kagglehub) (0.4.6)\n",
      "Downloading kagglehub-0.3.10-py3-none-any.whl (63 kB)\n",
      "Downloading PyYAML-6.0.2-cp313-cp313-win_amd64.whl (156 kB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm, pyyaml, kagglehub\n",
      "Successfully installed kagglehub-0.3.10 pyyaml-6.0.2 tqdm-4.67.1\n"
     ]
    }
   ],
   "source": [
    "! pip install kagglehub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\data pipeline\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: C:\\Users\\koppo\\.cache\\kagglehub\\datasets\\olistbr\\brazilian-ecommerce\\versions\\2\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"olistbr/brazilian-ecommerce\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"]=\"C:/Users/koppo/Downloads/valued-ceiling-454014-a9-f6887630e113.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/Users/koppo/Downloads/valued-ceiling-454014-a9-f6887630e113.json\n"
     ]
    }
   ],
   "source": [
    "#environment variables\n",
    "print(os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = bigquery.Client()\n",
    "csv_file = \"./data/olist_customers_dataset.csv\"\n",
    "csv_file = \"./data/olist_geolocation_dataset.csv\"\n",
    "csv_file = \"./data/olist_order_items_dataset.csv\"\n",
    "csv_file = \"./data/olist_order_payments_dataset.csv.csv\"\n",
    "csv_file = \"./data/olist_order_reviews_dataset.csv\"\n",
    "csv_file = \"./data/olist_orders_dataset.csv\"\n",
    "csv_file = \"./data/olist_products_dataset.csv\"\n",
    "csv_file = \"./data/olist_sellers_dataset.csvolist_products_dataset.csv\"\n",
    "csv_file = \"./data/data\\product_category_name_translation.csv.csv\"\n",
    "df = pd.read_csv(csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000163\n"
     ]
    }
   ],
   "source": [
    "print(df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   geolocation_zip_code_prefix  geolocation_lat  geolocation_lng  \\\n",
      "0                         1037       -23.545621       -46.639292   \n",
      "1                         1046       -23.546081       -46.644820   \n",
      "2                         1046       -23.546129       -46.642951   \n",
      "3                         1041       -23.544392       -46.639499   \n",
      "4                         1035       -23.541578       -46.641607   \n",
      "\n",
      "  geolocation_city geolocation_state  \n",
      "0        sao paulo                SP  \n",
      "1        sao paulo                SP  \n",
      "2        sao paulo                SP  \n",
      "3        sao paulo                SP  \n",
      "4        sao paulo                SP  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "df.dropna(how=\"all\", inplace=True)  # Remove fully empty records\n",
    "df.drop_duplicates(inplace=True)  # Remove duplicate records\n",
    "df.rename(columns={\"customer_id\": \"CustomerID\"}, inplace=True)  # Rename columns for consistency\n",
    "\n",
    "print(df.head())  # Preview data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "738332\n"
     ]
    }
   ],
   "source": [
    "print(df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CustomerID', 'customer_unique_id', 'customer_zip_code_prefix', 'customer_city', 'customer_state']\n",
      "{'CustomerID': 'STRING', 'customer_unique_id': 'STRING', 'customer_zip_code_prefix': 'INTEGER', 'customer_city': 'STRING', 'customer_state': 'STRING'}\n"
     ]
    }
   ],
   "source": [
    "column_names = df.columns.tolist()\n",
    "print(column_names)\n",
    "import numpy as np\n",
    "type_mapping = {\n",
    "    str: \"STRING\",\n",
    "    'int64': \"INTEGER\",\n",
    "    np.int64: \"INTEGER\",\n",
    "    float: \"FLOAT\",\n",
    "    bool: \"BOOLEAN\"\n",
    "}\n",
    "\n",
    "data_types_dict = {\n",
    "    col: type_mapping.get(type(df[col].iloc[0]), \"UNKNOWN\") for col in df.columns\n",
    "}\n",
    "\n",
    "print(data_types_dict)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SchemaField('CustomerID', 'STRING', 'NULLABLE', None, None, (), None), SchemaField('customer_unique_id', 'STRING', 'NULLABLE', None, None, (), None), SchemaField('customer_zip_code_prefix', 'INTEGER', 'NULLABLE', None, None, (), None), SchemaField('customer_city', 'STRING', 'NULLABLE', None, None, (), None), SchemaField('customer_state', 'STRING', 'NULLABLE', None, None, (), None)]\n"
     ]
    }
   ],
   "source": [
    "table_id = \"valued-ceiling-454014.student.olist_customers\"  # ProjectID.Dataset.TableName\n",
    "schema=[]\n",
    "for  key,value in data_types_dict.items():\n",
    "    schema.append(bigquery.SchemaField(key,value))\n",
    "\n",
    "# schema = [\n",
    "#         bigquery.SchemaField(\"CustomerID\", \"STRING\"),\n",
    "#         bigquery.SchemaField(\"customer_unique_id\", \"STRING\"),\n",
    "#         bigquery.SchemaField(\"customer_zip_code_prefix\", \"INTEGER\"),\n",
    "#         bigquery.SchemaField(\"customer_city\", \"STRING\"),\n",
    "#         bigquery.SchemaField(\"customer_state\", \"STRING\"),\n",
    "#     ]\n",
    "print(schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset valued-ceiling-454014-a9.sample already exists.\n"
     ]
    }
   ],
   "source": [
    "dataset_id = \"valued-ceiling-454014-a9.sample\"  # Corrected Dataset ID\n",
    "\n",
    "try:\n",
    "    client.get_dataset(dataset_id)  # Check if dataset exists\n",
    "    print(f\"Dataset {dataset_id} already exists.\")\n",
    "except:\n",
    "    dataset = bigquery.Dataset(dataset_id)\n",
    "    dataset.location = \"asia-south1\"  # Ensure correct location\n",
    "    client.create_dataset(dataset, exists_ok=True)\n",
    "    print(f\"Dataset {dataset_id} created.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "This method requires pyarrow to be installed",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[50]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m TABLE_ID = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mPROJECT_ID\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mDATASET_ID\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.olist_customers\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      4\u001b[39m job_config = bigquery.LoadJobConfig(schema=schema, write_disposition=\u001b[33m\"\u001b[39m\u001b[33mWRITE_TRUNCATE\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m job = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_table_from_dataframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTABLE_ID\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjob_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mjob_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mData successfully loaded into BigQuery!\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\data pipeline\\venv\\Lib\\site-packages\\google\\cloud\\bigquery\\client.py:2752\u001b[39m, in \u001b[36mClient.load_table_from_dataframe\u001b[39m\u001b[34m(self, dataframe, destination, num_retries, job_id, job_id_prefix, location, project, job_config, parquet_compression, timeout)\u001b[39m\n\u001b[32m   2744\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   2745\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mGot unexpected source_format: \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m. Currently, only PARQUET and CSV are supported\u001b[39m\u001b[33m\"\u001b[39m.format(\n\u001b[32m   2746\u001b[39m             new_job_config.source_format\n\u001b[32m   2747\u001b[39m         )\n\u001b[32m   2748\u001b[39m     )\n\u001b[32m   2750\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m pyarrow \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m new_job_config.source_format == job.SourceFormat.PARQUET:\n\u001b[32m   2751\u001b[39m     \u001b[38;5;66;03m# pyarrow is now the only supported parquet engine.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2752\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mThis method requires pyarrow to be installed\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   2754\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m location \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2755\u001b[39m     location = \u001b[38;5;28mself\u001b[39m.location\n",
      "\u001b[31mValueError\u001b[39m: This method requires pyarrow to be installed"
     ]
    }
   ],
   "source": [
    "PROJECT_ID = \"valued-ceiling-454014-a9\"\n",
    "DATASET_ID = \"sample\"\n",
    "TABLE_ID = f\"{PROJECT_ID}.{DATASET_ID}.olist_customers\"\n",
    "job_config = bigquery.LoadJobConfig(schema=schema, write_disposition=\"WRITE_TRUNCATE\")\n",
    "\n",
    "job = client.load_table_from_dataframe(df, TABLE_ID, job_config=job_config)\n",
    "print(\"Data successfully loaded into BigQuery!\")\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
